{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71191f06",
   "metadata": {},
   "source": [
    "# Installing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e978ae1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install coremltools\n",
    "# !pip install -U 'comet-ml>=3.31.0'\n",
    "# !pip install pycocotools\n",
    "# !pip install transformers\n",
    "# !pip install scikit-learn\n",
    "# !pip install pytorch-lightning\n",
    "# !pip install torch torchvision torchaudio\n",
    "\n",
    "# conda install -c conda-forge gcc libstdcxx-ng"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d013bec9",
   "metadata": {},
   "source": [
    "# Importing required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ba2e9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scikit-learn version 1.4.2 is not supported. Minimum required version: 0.17. Maximum required version: 1.1.2. Disabling scikit-learn conversion API.\n",
      "Torch version 2.3.0+cu121 has not been tested with coremltools. You may run into unexpected errors. Torch 2.2.0 is the most recent version that has been tested.\n",
      "Failed to load _MLModelProxy: No module named 'coremltools.libcoremlpython'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import joblib\n",
    "import numpy as np\n",
    "import torchmetrics\n",
    "import torch.nn as nn\n",
    "import coremltools as ct\n",
    "from datetime import datetime\n",
    "import pytorch_lightning as pl\n",
    "from PIL import Image,ImageDraw\n",
    "import torch.nn.functional as F\n",
    "from pycocotools.coco import COCO\n",
    "from matplotlib import pyplot as plt\n",
    "from pytorch_lightning import Trainer\n",
    "from torchmetrics import JaccardIndex\n",
    "import torchvision.transforms as transforms\n",
    "from pytorch_lightning.loggers import CometLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from transformers import SegformerForSemanticSegmentation\n",
    "from scipy.ndimage import label, find_objects, distance_transform_edt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0fb892",
   "metadata": {},
   "source": [
    "# Setting SLURM variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3022677",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['SLURM_NTASKS_PER_NODE'] = '8'\n",
    "os.environ.pop('SLURM_NTASKS', None)\n",
    "\n",
    "# Print SLURM environment variables to verify\n",
    "# print(\"SLURM Environment Variables:\")\n",
    "# print(\"SLURM_JOB_ID:\", os.environ.get('SLURM_JOB_ID', 'Not Set'))\n",
    "# print(\"SLURM_NTASKS:\", os.environ.get('SLURM_NTASKS', 'Not Set'))\n",
    "# print(\"SLURM_NTASKS_PER_NODE:\", os.environ.get('SLURM_NTASKS_PER_NODE', 'Not Set'))\n",
    "# print(\"SLURM_JOB_NODELIST:\", os.environ.get('SLURM_JOB_NODELIST', 'Not Set'))\n",
    "# print(\"SLURM_JOB_NAME:\", os.environ.get('SLURM_JOB_NAME', 'Not Set'))\n",
    "\n",
    "# Set float32 matmul precision for Tensor Cores\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f95670",
   "metadata": {},
   "source": [
    "# Defining file locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e11fcbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image paths\n",
    "test_dir = './images/merged_test'\n",
    "train_dir = './images/merged_train'\n",
    "val_dir = './images/merged_val'\n",
    "\n",
    "#Annotation Paths\n",
    "test_ann_file = './annotations/merged_test_01_22_25.json'\n",
    "train_ann_file = './annotations/merged_train_01_22_25.json'\n",
    "val_ann_file = './annotations/merged_val_01_22_25.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c90e6ee",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a2ff0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def dynamic_normalization(img_dir, annotation_file):\n",
    "#     coco = COCO(annotation_file)\n",
    "#     img_ids = list(coco.imgs.keys())\n",
    "#     means = []\n",
    "#     stds = []\n",
    "\n",
    "#     for img_id in img_ids:\n",
    "#         img_info = coco.imgs[img_id]\n",
    "#         path = os.path.join(img_dir, img_info['file_name'])\n",
    "#         image = Image.open(path).convert('RGB')\n",
    "\n",
    "#         # Convert image to tensor without normalization\n",
    "#         to_tensor = transforms.ToTensor()\n",
    "#         image_tensor = to_tensor(image)\n",
    "        \n",
    "#         means.append(image_tensor.mean(dim=(1, 2)))\n",
    "#         stds.append(image_tensor.std(dim=(1, 2)))\n",
    "\n",
    "#     mean = torch.stack(means).mean(dim=0)\n",
    "#     std = torch.stack(stds).mean(dim=0)\n",
    "    \n",
    "#         # Print or log the mean and std\n",
    "#     print(f\"Calculated mean: {mean.tolist()}\")\n",
    "#     print(f\"Calculated std: {std.tolist()}\")\n",
    "\n",
    "#     return transforms.Compose([\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean, std),\n",
    "#     ])\n",
    "\n",
    "# Normalization transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a6fbfd",
   "metadata": {},
   "source": [
    "# Defining COCO loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8162d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loader for COCO formatted data\n",
    "def get_coco_loader(img_dir, annotation_file, transform, augment = False, num_augmentations = 1):\n",
    "    coco = COCO(annotation_file)\n",
    "    img_ids = list(coco.imgs.keys())\n",
    "    def loader():\n",
    "        for img_id in img_ids:\n",
    "            img_info = coco.imgs[img_id]\n",
    "            path = os.path.join(img_dir, img_info['file_name'])\n",
    "            image = Image.open(path).convert('RGB')\n",
    "            image_tensor = transform(image)\n",
    "            ann_ids = coco.getAnnIds(imgIds=[img_id])\n",
    "            anns = coco.loadAnns(ann_ids)\n",
    "            mask = np.zeros((img_info['height'], img_info['width']), dtype=np.int64)\n",
    "            for ann in anns:\n",
    "                mask = np.maximum(mask, coco.annToMask(ann) * ann['category_id'])\n",
    "            mask_tensor = torch.tensor(mask)\n",
    "            yield image_tensor, mask_tensor\n",
    "            \n",
    "            if augment:\n",
    "                for _ in range(num_augmentations):\n",
    "                    aug_image, aug_mask = apply_transform(image, mask)\n",
    "                    aug_image_tensor = transform(aug_image)\n",
    "                    aug_mask_tensor = torch.tensor(aug_mask, dtype=torch.int64)\n",
    "                    yield aug_image_tensor, aug_mask_tensor\n",
    "    return loader\n",
    "\n",
    "def get_position_transform():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),  \n",
    "        transforms.RandomRotation(15),      \n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "def get_color_transform():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),  # Randomly changes the brightness, contrast, saturation, and hue\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "def apply_transform(image, mask):\n",
    "    # Ensure the mask is in uint8 format then convert to PIL Image\n",
    "    mask = mask.astype(np.uint8)  \n",
    "    mask = Image.fromarray(mask) \n",
    "\n",
    "    # seed for random transformations\n",
    "    seed = torch.randint(0, 2**32, (1,)).item()\n",
    "\n",
    "    # Apply the same position transformation to both image and mask\n",
    "    torch.manual_seed(seed)\n",
    "    image = get_position_transform()(image)\n",
    "    torch.manual_seed(seed)\n",
    "    mask = get_position_transform()(mask)\n",
    "\n",
    "    # apply color transformation to image\n",
    "    image = get_color_transform()(image)\n",
    "\n",
    "    # Convert mask back to numpy array\n",
    "    mask = np.array(mask)\n",
    "\n",
    "    return image, mask\n",
    "\n",
    "# Custom DataLoader for COCO data\n",
    "class COCOLoader(IterableDataset):\n",
    "    def __init__(self, img_dir, annotation_file, transform, augment=False, num_augmentations=1):\n",
    "        super().__init__()\n",
    "        self.img_dir = img_dir\n",
    "        self.annotation_file = annotation_file\n",
    "        self.transform = transform\n",
    "        self.augment = augment\n",
    "        self.num_augmentations = num_augmentations\n",
    "\n",
    "    def __iter__(self):\n",
    "        return get_coco_loader(self.img_dir, self.annotation_file, self.transform, self.augment, self.num_augmentations)()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e8f4d3",
   "metadata": {},
   "source": [
    "# Defining Test-Train-Val loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebf104f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setup data tranforms\n",
    "# test_transform = dynamic_normalization(test_dir, test_ann_file)\n",
    "# train_transform = dynamic_normalization(train_dir, train_ann_file)\n",
    "# val_transform = dynamic_normalization(val_dir, val_ann_file)\n",
    "\n",
    "\n",
    "# # Setup data loaders\n",
    "test_loader = DataLoader(COCOLoader(test_dir, test_ann_file, transform), batch_size = 128, num_workers = 4)\n",
    "train_loader = DataLoader(COCOLoader(train_dir, train_ann_file, transform), batch_size = 128, num_workers = 4)\n",
    "val_loader = DataLoader(COCOLoader(val_dir, val_ann_file, transform), batch_size = 128, num_workers = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d72dc60",
   "metadata": {},
   "source": [
    "# Visualize Images with annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c77f7330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for images, masks in val_loader:\n",
    "#     # Convert tensors to numpy arrays\n",
    "#     images_np = images.permute(0, 2, 3, 1).numpy()  # Convert from (N, C, H, W) to (N, H, W, C)\n",
    "#     masks_np = masks.numpy()\n",
    "\n",
    "#     # Plot images and masks\n",
    "#     for i in range(len(images)):\n",
    "#         image_np = images_np[i]\n",
    "#         mask_np = masks_np[i]\n",
    "\n",
    "#         # Overlay mask on image\n",
    "#         plt.figure(figsize=(10, 5))\n",
    "#         plt.subplot(1, 2, 2)\n",
    "#         plt.imshow(image_np)\n",
    "#         plt.imshow(mask_np, alpha=0.5, cmap='jet', interpolation='nearest')  # Overlay mask on image\n",
    "#         plt.title('Image with Pupil Highlighted')\n",
    "#         plt.axis('off')\n",
    "\n",
    "#         plt.show()\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c9151a",
   "metadata": {},
   "source": [
    "# Segformer Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce8d98a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegformerWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(SegformerWrapper, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, pixel_values, labels=None):\n",
    "        outputs = self.model(pixel_values=pixel_values, labels=labels)\n",
    "        logits = outputs.logits if isinstance(outputs, dict) else outputs\n",
    "        return logits\n",
    "\n",
    "class SegformerModule(pl.LightningModule):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "            'nvidia/segformer-b0-finetuned-ade-512-512',\n",
    "            num_labels=num_classes,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        self.wrapper = SegformerWrapper(self.model)\n",
    "        self.test_losses = []\n",
    "        self.test_ious = []\n",
    "        self.metrics = torchmetrics.JaccardIndex(task='multiclass', num_classes=self.model.config.num_labels, average='macro')\n",
    "\n",
    "          #Initialize example input array with a dummy input\n",
    "        self.example_input_array = torch.randn(1, 3, 640, 480)\n",
    "\n",
    "    def forward(self, pixel_values, labels=None):\n",
    "        return self.wrapper(pixel_values=pixel_values, labels=labels)\n",
    "\n",
    "    def compute_loss(self, outputs, labels):\n",
    "        logits = outputs\n",
    "        logits = nn.functional.interpolate(logits, size=labels.shape[-2:], mode='bilinear', align_corners=False)\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return loss, logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self(images, labels=labels)\n",
    "        loss, logits = self.compute_loss(outputs, labels)\n",
    "        self.log('train_loss', loss)\n",
    "\n",
    "        preds = torch.argmax(logits, dim = 1)\n",
    "        self.metrics.update(preds, labels)\n",
    "        self.log('train_iou', self.metrics, on_step = False, on_epoch = True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self(images, labels=labels)\n",
    "        val_loss, logits = self.compute_loss(outputs, labels)\n",
    "        self.log('val_loss', val_loss)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.metrics.update(preds, labels)\n",
    "        self.log('val_iou', self.metrics, on_step=False, on_epoch=True)\n",
    "\n",
    "        return val_loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self(images, labels=labels)\n",
    "        loss, logits = self.compute_loss(outputs, labels)\n",
    "        self.log('test_loss', loss)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.metrics.update(preds, labels)\n",
    "        self.log('test_iou', self.metrics, on_step=False, on_epoch=True)\n",
    "\n",
    "        self.test_losses.append(loss.detach())\n",
    "        return {'test_loss': loss}\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        avg_loss = torch.stack(self.test_losses).mean()\n",
    "        avg_iou = self.metrics.compute()  # Compute the final IoU score\n",
    "        self.log('avg_test_loss', avg_loss)\n",
    "        self.log('avg_iou_score', avg_iou)\n",
    "        self.test_losses.clear()\n",
    "        self.test_ious.clear()\n",
    "        self.metrics.reset()  # Reset metric states for the next epoch\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=0.0001)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09b9c0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check If cuda is available\n",
    "# print(torch.cuda.is_available())\n",
    "# print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2319528f",
   "metadata": {},
   "source": [
    "# Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9edeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and checkpoint\n",
    "num_classes = 2 \n",
    "segformer = SegformerModule(num_classes)\n",
    "checkpoint_callback = ModelCheckpoint(monitor='val_loss', mode='min', dirpath='./checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca86d6d4",
   "metadata": {},
   "source": [
    "# Defining trainer attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc3e13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(    \n",
    "        max_epochs = 128,\n",
    "        accelerator = 'gpu' if torch.cuda.is_available() else 'cpu',\n",
    "        devices = 1, \n",
    "        callbacks = [checkpoint_callback]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d7d060",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251f16e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.fit(segformer, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a08b67",
   "metadata": {},
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15195f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "trainer.test(model = segformer, dataloaders = test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c339ee91",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af349014",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = './checkpoints/segformer_model.pth'\n",
    "torch.save(segformer.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1fbe0c",
   "metadata": {},
   "source": [
    "# Loading model from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c293c5ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Assuming `checkpoint_callback.best_model_path` contains the path to the best model checkpoint\n",
    "model_path = checkpoint_callback.best_model_path\n",
    "segformer = SegformerModule.load_from_checkpoint(model_path, num_classes=2)\n",
    "segformer.eval()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "segformer.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b9593b",
   "metadata": {},
   "source": [
    "# Visualizing model's predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8be06b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to de-normalize the image\n",
    "# def denormalize(image_tensor, mean, std):\n",
    "#     image_np = image_tensor.permute(1, 2, 0).cpu().numpy()  # Convert to HWC format\n",
    "#     mean = np.array(mean)\n",
    "#     std = np.array(std)\n",
    "#     image_np = std * image_np + mean  # De-normalize\n",
    "#     image_np = np.clip(image_np * 255, 0, 255).astype(np.uint8)  # Convert to uint8\n",
    "#     return image_np\n",
    "\n",
    "def denormalize(image_tensor):\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    image_np = image_tensor.permute(1, 2, 0).cpu().numpy()  # Convert to HWC format\n",
    "    image_np = std * image_np + mean  # De-normalize\n",
    "    image_np = np.clip(image_np * 255, 0, 255).astype(np.uint8)  # Convert to uint8\n",
    "    return image_np\n",
    "\n",
    "color_map = {\n",
    "    0: (0, 0, 0),   # Background\n",
    "    1: (255, 0, 0), # Original mask\n",
    "    2: (0, 255, 0), # Predicted mask\n",
    "}\n",
    "\n",
    "# Define mean and std for de-normalization\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Ensure your existing model instance is in evaluation mode\n",
    "segformer.eval()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "segformer.to(device)  # Move the model to the appropriate device\n",
    "\n",
    "# Function to overlay mask\n",
    "def overlay_mask(image_np, mask_np, color):\n",
    "    vis_shape = image_np.shape\n",
    "    overlay = image_np.copy()\n",
    "    for i in range(3):\n",
    "        overlay[:, :, i] = np.where(mask_np == 1, color[i], overlay[:, :, i])\n",
    "    return Image.blend(Image.fromarray(image_np), Image.fromarray(overlay), alpha=0.5)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, masks in test_loader:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        outputs = segformer(images)\n",
    "        logits = outputs.logits if hasattr(outputs, 'logits') else outputs\n",
    "\n",
    "        upsampled_logits = torch.nn.functional.interpolate(logits, size=masks.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "        predicted = upsampled_logits.argmax(dim=1).cpu().numpy()\n",
    "        masks = masks.cpu().numpy()\n",
    "\n",
    "        for i in range(len(images)):\n",
    "            image_np = denormalize(images[i], mean, std)\n",
    "            original_mask_np = masks[i]\n",
    "            pred_mask_np = predicted[i]\n",
    "\n",
    "            original_overlay = overlay_mask(image_np, original_mask_np, color_map[1])\n",
    "            predicted_overlay = overlay_mask(image_np, pred_mask_np, color_map[1])\n",
    "\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.imshow(original_overlay)\n",
    "            plt.title('Image with Original Mask')\n",
    "            plt.axis('off')\n",
    "\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.imshow(predicted_overlay)\n",
    "            plt.title('Image with Predicted Mask')\n",
    "            plt.axis('off')\n",
    "\n",
    "            plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf40461a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "insight2024",
   "language": "python",
   "name": "insight2024"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
